{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce npy dataset required for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/c2p3vkxd5sg6dqhwf1q7m4pm0000gn/T/ipykernel_96305/3720829906.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../../data/pt_decoding_data_S62.pkl\n",
      "Type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Load the pkl file\n",
    "pkl_file = '../../data/pt_decoding_data_S62.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded: {pkl_file}\")\n",
    "print(f\"Type: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['S14', 'S26', 'S23', 'S33', 'S22', 'S39', 'S58', 'S62'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ID', 'X1', 'X1_map', 'y1', 'X2', 'X2_map', 'y2', 'X3', 'X3_map', 'y3', 'y_full_phon', 'X_collapsed', 'y_phon_collapsed', 'y_artic_collapsed', 'pre_pts'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['S14'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 8, 16, 200)\n",
      "(148, 8, 16, 200)\n",
      "(151, 8, 16, 200)\n",
      "(46, 12, 24, 200)\n",
      "(151, 8, 16, 200)\n",
      "(137, 24, 12, 200)\n",
      "(141, 24, 12, 200)\n",
      "(178, 24, 12, 200)\n"
     ]
    }
   ],
   "source": [
    "for patient_name in data.keys():\n",
    "    print(data[patient_name]['X1_map'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coords, x_coords = np.meshgrid(\n",
    "    np.linspace(0, 1, 8),\n",
    "    np.linspace(0, 1, 16),\n",
    "    indexing=\"ij\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_positions = np.column_stack([x_coords.ravel(), y_coords.ravel()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train.npy, y_train.npy, X_test.npy, y_test.npy [test = one patient, train = the rest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first unify the channels across patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def spatial_resample(data, tgt_H, tgt_W):\n",
    "    \"\"\"\n",
    "    Resample 4D neural data (B, H, W, T) to a target spatial grid (tgt_H, tgt_W).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Input array of shape (B, H, W, T)\n",
    "    tgt_H, tgt_W : int\n",
    "        Target grid height and width\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Resampled data of shape (B, tgt_H * tgt_W, T)\n",
    "    \"\"\"\n",
    "\n",
    "    B, H, W, T = data.shape\n",
    "\n",
    "    # 1Ô∏è‚É£ Flatten the spatial grid -> (B, H*W, T)\n",
    "    x = data.reshape(B, H * W, T)\n",
    "\n",
    "    # 2Ô∏è‚É£ Build coordinate grids for the original layout (normalized 0‚Äì1)\n",
    "    y_coords, x_coords = np.meshgrid(\n",
    "        np.linspace(0, 1, H),\n",
    "        np.linspace(0, 1, W),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    actual_positions = np.column_stack([x_coords.ravel(), y_coords.ravel()])\n",
    "\n",
    "    # 3Ô∏è‚É£ Build coordinate grids for the target layout\n",
    "    y_t, x_t = np.meshgrid(\n",
    "        np.linspace(0, 1, tgt_H),\n",
    "        np.linspace(0, 1, tgt_W),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    target_positions = np.column_stack([x_t.ravel(), y_t.ravel()])\n",
    "\n",
    "    # 4Ô∏è‚É£ Compute inverse-distance interpolation weights\n",
    "    dist = cdist(target_positions, actual_positions)\n",
    "    weights = 1 / (dist + 1e-6)\n",
    "    weights /= weights.sum(axis=1, keepdims=True)  # normalize each row\n",
    "\n",
    "    # 5Ô∏è‚É£ Map each trial into the target grid\n",
    "    x_mapped = np.zeros((B, tgt_H * tgt_W, T))\n",
    "    for b in range(B):\n",
    "        x_mapped[b] = weights @ x[b]\n",
    "\n",
    "    return x_mapped.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "def EA(x):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array\n",
    "        data of shape (num_samples, num_channels, num_time_samples)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    XEA : numpy array\n",
    "        data of shape (num_samples, num_channels, num_time_samples)\n",
    "    \"\"\"\n",
    "    cov = np.zeros((x.shape[0], x.shape[1], x.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        cov[i] = np.cov(x[i])\n",
    "    refEA = np.mean(cov, 0)\n",
    "    sqrtRefEA = fractional_matrix_power(refEA, -0.5) \n",
    "    XEA = np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]):\n",
    "        XEA[i] = np.dot(sqrtRefEA, x[i])\n",
    "    return XEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {k: [] for k in data.keys()}\n",
    "\n",
    "for patient_name in data.keys():\n",
    "    original_patient_data = data[patient_name]['X1_map']\n",
    "    processed_patient_data = spatial_resample(original_patient_data, 16, 8)\n",
    "    processed_data[patient_name] = processed_patient_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create test set & train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient = 'S14'\n",
    "\n",
    "X_test = processed_data[test_patient]\n",
    "y_test = data[test_patient]['y1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 128, 200)\n",
      "(144,)\n",
      "(1096, 128, 200)\n",
      "(1096,)\n",
      "Label range: 0 to 8\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for patient_name in data.keys():\n",
    "\n",
    "    y_train.append(data[patient_name]['y1'])\n",
    "    X_train.append(processed_data[patient_name])\n",
    "\n",
    "# Concatenate all patients along the trial axis\n",
    "X_train = np.concatenate(X_train, axis=0)  # (total_patients, trials, chan, time) -> (total_trials, chan, time)\n",
    "y_train = np.concatenate(y_train, axis=0)  # (total_patients, trials) -> (total_trials,)\n",
    "\n",
    "# Convert labels from 1‚Äì9 ‚Üí 0‚Äì8\n",
    "y_train = y_train - 1\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(f\"Label range: {y_train.min()} to {y_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: (1096, 128, 200) (1096,)\n"
     ]
    }
   ],
   "source": [
    "# ==== CONFIG ====\n",
    "DATASET_NAME = \"basic\"\n",
    "WEIGHT_PATH = \"./weight/MIRepNet.pth\"   # pretrained weights (4-class)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ==== LOAD DATA ====\n",
    "X = np.load(f'../../data/{DATASET_NAME}/X.npy')   # (N, 111, 200)\n",
    "y = np.load(f'../../data/{DATASET_NAME}/labels.npy')  # (N,)\n",
    "print(\"Loaded data:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/X.npy', X_train)\n",
    "np.save('../../data/labels.npy', y_train)\n",
    "np.save('../../data/X_test.npy', X_test)\n",
    "np.save('../../data/labels_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create per-patient datasets (phoneme 1 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "üìä Original data size (before augmentation):\n",
      "  train: (80, 128, 200)\n",
      "  val:   (20, 128, 200)\n",
      "  test:  (44, 128, 200)\n",
      "\n",
      "üìà After augmentation:\n",
      "  train: (800, 128, 200)\n",
      "  val:   (20, 128, 200)\n",
      "  test:  (44, 128, 200)\n",
      "===============================================\n",
      "\n",
      "üìÅ Files saved to:\n",
      "  training data (post aug): /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/X_train.npy\n",
      "  training labels (post aug): /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/labels_train.npy\n",
      "  validation data: /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/X_val.npy\n",
      "  validation labels: /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/labels_val.npy\n",
      "  testing data: /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/X_test.npy\n",
      "  testing labels: /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/labels_test.npy\n",
      "  training data (before aug): /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/X_same_training_data_before_aug.npy\n",
      "  training labels (before aug): /Users/wangmaidou/Documents/EEG-Model-Fine-tune/data/S14/labels_same_training_data_before_aug.npy\n",
      "\n",
      "‚ú® Data augmented to 800 training trials (x10 increase)\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# ==== CONFIG ====\n",
    "current_patient = 'S14'\n",
    "test_proportion = 0.3\n",
    "val_proportion = 0.2       # 20% of train data goes to validation\n",
    "augmentation_factor = 10   # how many times to augment the training set\n",
    "augmentation_strength = 0.5  # 0 = identical copies, 1 = strong augmentation\n",
    "\n",
    "# ==== LOAD ORIGINAL EEG DATA ====\n",
    "all_trials = data[current_patient]['X1_map']\n",
    "all_trials = all_trials.reshape(\n",
    "    all_trials.shape[0],\n",
    "    all_trials.shape[1] * all_trials.shape[2],\n",
    "    all_trials.shape[3]\n",
    ")\n",
    "all_labels = data[current_patient]['y1'] - 1  # shift labels to start at 0\n",
    "\n",
    "# ==== TRAIN/TEST SPLIT ====\n",
    "train_trials, test_trials, train_labels, test_labels = train_test_split(\n",
    "    all_trials, all_labels,\n",
    "    test_size=test_proportion,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ==== TRAIN/VAL SPLIT (from train only) ====\n",
    "train_trials, val_trials, train_labels, val_labels = train_test_split(\n",
    "    train_trials, train_labels,\n",
    "    test_size=val_proportion,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ==== SAVE ORIGINAL (UN-AUGMENTED) TRAINING COPY ====\n",
    "save_dir = f'../../data/{current_patient}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "pre_aug_train_X_path = f\"{save_dir}/X_same_training_data_before_aug.npy\"\n",
    "pre_aug_train_y_path = f\"{save_dir}/labels_same_training_data_before_aug.npy\"\n",
    "np.save(pre_aug_train_X_path, train_trials)\n",
    "np.save(pre_aug_train_y_path, train_labels)\n",
    "\n",
    "\n",
    "# ==== DATA AUGMENTATION FUNCTION ====\n",
    "def augment_eeg_data(X, strength=0.5):\n",
    "    \"\"\"Random temporal shift, amplitude scaling, and Gaussian noise.\n",
    "       strength ‚àà [0, 1] controls how drastic the augmentations are.\"\"\"\n",
    "    X_aug = np.copy(X)\n",
    "    max_shift = int(10 * strength)\n",
    "    shift_vals = np.random.randint(-max_shift, max_shift + 1, size=X.shape[0])\n",
    "    for i, s in enumerate(shift_vals):\n",
    "        X_aug[i] = np.roll(X_aug[i], s, axis=-1)\n",
    "    \n",
    "    scale_low, scale_high = 1 - 0.1 * strength, 1 + 0.1 * strength\n",
    "    scale_factors = np.random.uniform(scale_low, scale_high, size=X.shape[0])\n",
    "    X_aug *= scale_factors[:, np.newaxis, np.newaxis]\n",
    "    \n",
    "    noise_std = 0.01 * strength\n",
    "    noise = np.random.normal(0, noise_std, size=X.shape)\n",
    "    X_aug += noise.astype(np.float32)\n",
    "    \n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# ==== APPLY AUGMENTATION ONLY TO TRAINING SET ====\n",
    "augmented_trials, augmented_labels = [], []\n",
    "for _ in range(augmentation_factor):\n",
    "    X_aug = augment_eeg_data(train_trials, strength=augmentation_strength)\n",
    "    augmented_trials.append(X_aug)\n",
    "    augmented_labels.append(train_labels)\n",
    "\n",
    "train_trials = np.concatenate(augmented_trials, axis=0)\n",
    "train_labels = np.concatenate(augmented_labels, axis=0)\n",
    "\n",
    "\n",
    "# ==== SAVE FINAL FILES ====\n",
    "train_X_path = f'{save_dir}/X_train.npy'\n",
    "train_y_path = f'{save_dir}/labels_train.npy'\n",
    "val_X_path   = f'{save_dir}/X_val.npy'\n",
    "val_y_path   = f'{save_dir}/labels_val.npy'\n",
    "test_X_path  = f'{save_dir}/X_test.npy'\n",
    "test_y_path  = f'{save_dir}/labels_test.npy'\n",
    "\n",
    "np.save(train_X_path, train_trials)\n",
    "np.save(train_y_path, train_labels)\n",
    "np.save(val_X_path, val_trials)\n",
    "np.save(val_y_path, val_labels)\n",
    "np.save(test_X_path, test_trials)\n",
    "np.save(test_y_path, test_labels)\n",
    "\n",
    "\n",
    "# ==== PRINT SUMMARY ====\n",
    "abs_dir = os.path.abspath(save_dir)\n",
    "print(\"===============================================\")\n",
    "print(\"üìä Original data size (before augmentation):\")\n",
    "print(f\"  train: {np.load(pre_aug_train_X_path).shape}\")\n",
    "print(f\"  val:   {val_trials.shape}\")\n",
    "print(f\"  test:  {test_trials.shape}\")\n",
    "\n",
    "print(\"\\nüìà After augmentation:\")\n",
    "print(f\"  train: {train_trials.shape}\")\n",
    "print(f\"  val:   {val_trials.shape}\")\n",
    "print(f\"  test:  {test_trials.shape}\")\n",
    "print(\"===============================================\")\n",
    "\n",
    "print(\"\\nüìÅ Files saved to:\")\n",
    "print(f\"  training data (post aug): {os.path.abspath(train_X_path)}\")\n",
    "print(f\"  training labels (post aug): {os.path.abspath(train_y_path)}\")\n",
    "print(f\"  validation data: {os.path.abspath(val_X_path)}\")\n",
    "print(f\"  validation labels: {os.path.abspath(val_y_path)}\")\n",
    "print(f\"  testing data: {os.path.abspath(test_X_path)}\")\n",
    "print(f\"  testing labels: {os.path.abspath(test_y_path)}\")\n",
    "print(f\"  training data (before aug): {os.path.abspath(pre_aug_train_X_path)}\")\n",
    "print(f\"  training labels (before aug): {os.path.abspath(pre_aug_train_y_path)}\")\n",
    "\n",
    "print(f\"\\n‚ú® Data augmented to {train_trials.shape[0]} training trials (x{augmentation_factor} increase)\")\n",
    "print(\"===============================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
