{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce npy dataset required for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/c2p3vkxd5sg6dqhwf1q7m4pm0000gn/T/ipykernel_73292/2010542509.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: pt_decoding_data_S62.pkl\n",
      "Type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Load the pkl file\n",
    "pkl_file = 'pt_decoding_data_S62.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded: {pkl_file}\")\n",
    "print(f\"Type: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['S14', 'S26', 'S23', 'S33', 'S22', 'S39', 'S58', 'S62'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ID', 'X1', 'X1_map', 'y1', 'X2', 'X2_map', 'y2', 'X3', 'X3_map', 'y3', 'y_full_phon', 'X_collapsed', 'y_phon_collapsed', 'y_artic_collapsed', 'pre_pts'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['S14'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (24,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         y_item = data[ID][\u001b[33m'\u001b[39m\u001b[33my1\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m         y.append(y_item)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m X = X.reshape(X.shape[\u001b[32m0\u001b[39m] * X.shape[\u001b[32m1\u001b[39m], X.shape[\u001b[32m2\u001b[39m], X.shape[\u001b[32m3\u001b[39m])\n\u001b[32m     16\u001b[39m y = np.array(y)\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (24,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for ID in data:\n",
    "    for item in [data[ID]['X1'],data[ID]['X2'],data[ID]['X3']]:\n",
    "        X_item = data[ID]['X1']\n",
    "        X_item = np.transpose(X_item, (0, 2, 1)) # trials, channels, timepoints\n",
    "        X.append(X_item)\n",
    "    for item in [data[ID]['y1'],data[ID]['y2'],data[ID]['y3']]:\n",
    "        y_item = data[ID]['y1']\n",
    "        y.append(y_item)\n",
    "\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0] * X.shape[1], X.shape[2], X.shape[3])\n",
    "\n",
    "y = np.array(y)\n",
    "y = y.reshape(y.shape[0] * y.shape[1])\n",
    "\n",
    "# Convert labels from 1-9 to 0-8\n",
    "y = y - 1\n",
    "\n",
    "print(X.shape) # total trials (X1-3, all patients), channels, timesteps\n",
    "print(y.shape) # total trials\n",
    "print(f\"Label range: {y.min()} to {y.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy', X)\n",
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X.npy and y.npy\n",
      "X shape: (3456, 111, 200)\n",
      "y shape: (3456,)\n",
      "\n",
      "Label verification:\n",
      "Min label: 0\n",
      "Max label: 8\n",
      "Unique labels: [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "Label distribution:\n",
      "  Label 0: 408 samples\n",
      "  Label 1: 408 samples\n",
      "  Label 2: 408 samples\n",
      "  Label 3: 504 samples\n",
      "  Label 4: 240 samples\n",
      "  Label 5: 288 samples\n",
      "  Label 6: 360 samples\n",
      "  Label 7: 576 samples\n",
      "  Label 8: 264 samples\n"
     ]
    }
   ],
   "source": [
    "# Load the saved npy files to verify\n",
    "X_loaded = np.load('X.npy')\n",
    "y_loaded = np.load('y.npy')\n",
    "\n",
    "print(\"Loaded X.npy and y.npy\")\n",
    "print(f\"X shape: {X_loaded.shape}\")\n",
    "print(f\"y shape: {y_loaded.shape}\")\n",
    "print(f\"\\nLabel verification:\")\n",
    "print(f\"Min label: {y_loaded.min()}\")\n",
    "print(f\"Max label: {y_loaded.max()}\")\n",
    "print(f\"Unique labels: {np.unique(y_loaded)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "unique, counts = np.unique(y_loaded, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Label {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create per-patient datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: mydata_patients\n",
      "\n",
      "Processing S14...\n",
      "  X_S14.npy: shape (432, 111, 200)\n",
      "  y_S14.npy: shape (432,), labels 0-8\n",
      "\n",
      "Processing S26...\n",
      "  X_S26.npy: shape (444, 111, 200)\n",
      "  y_S26.npy: shape (444,), labels 0-8\n",
      "\n",
      "Processing S23...\n",
      "  X_S23.npy: shape (453, 63, 200)\n",
      "  y_S23.npy: shape (453,), labels 0-8\n",
      "\n",
      "Processing S33...\n",
      "  X_S33.npy: shape (138, 149, 200)\n",
      "  y_S33.npy: shape (138,), labels 0-8\n",
      "\n",
      "Processing S22...\n",
      "  X_S22.npy: shape (453, 74, 200)\n",
      "  y_S22.npy: shape (453,), labels 0-8\n",
      "\n",
      "Processing S39...\n",
      "  X_S39.npy: shape (411, 144, 200)\n",
      "  y_S39.npy: shape (411,), labels 0-8\n",
      "\n",
      "Processing S58...\n",
      "  X_S58.npy: shape (423, 171, 200)\n",
      "  y_S58.npy: shape (423,), labels 0-8\n",
      "\n",
      "Processing S62...\n",
      "  X_S62.npy: shape (534, 201, 200)\n",
      "  y_S62.npy: shape (534,), labels 0-8\n",
      "\n",
      "All patient data saved to mydata_patients/\n"
     ]
    }
   ],
   "source": [
    "# Create folder for per-patient data\n",
    "import os\n",
    "\n",
    "output_dir = 'mydata_patients'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Created directory: {output_dir}\\n\")\n",
    "\n",
    "# Process each patient\n",
    "for patient_id in data.keys():\n",
    "    print(f\"Processing {patient_id}...\")\n",
    "    \n",
    "    # Collect X data from X1, X2, X3\n",
    "    X_patient = []\n",
    "    for x_key in ['X1', 'X2', 'X3']:\n",
    "        X_item = data[patient_id][x_key]\n",
    "        X_item = np.transpose(X_item, (0, 2, 1))  # trials, channels, timepoints\n",
    "        X_patient.append(X_item)\n",
    "    \n",
    "    # Concatenate all trials\n",
    "    X_patient = np.concatenate(X_patient, axis=0)\n",
    "    \n",
    "    # Collect y data from y1, y2, y3\n",
    "    y_patient = []\n",
    "    for y_key in ['y1', 'y2', 'y3']:\n",
    "        y_item = data[patient_id][y_key]\n",
    "        y_patient.append(y_item)\n",
    "    \n",
    "    # Concatenate all labels\n",
    "    y_patient = np.concatenate(y_patient, axis=0)\n",
    "    \n",
    "    # Convert labels from 1-9 to 0-8\n",
    "    y_patient = y_patient - 1\n",
    "    \n",
    "    # Save to files\n",
    "    np.save(f'{output_dir}/X_{patient_id}.npy', X_patient)\n",
    "    np.save(f'{output_dir}/y_{patient_id}.npy', y_patient)\n",
    "    \n",
    "    print(f\"  X_{patient_id}.npy: shape {X_patient.shape}\")\n",
    "    print(f\"  y_{patient_id}.npy: shape {y_patient.shape}, labels {y_patient.min()}-{y_patient.max()}\\n\")\n",
    "\n",
    "print(f\"All patient data saved to {output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
